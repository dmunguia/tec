---
title: "Capítulo 5. Divide y conquista"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Marzo, 2019"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\Particionar}{Particionar}
    - \DeclareMathOperator{\Quicksort}{Quicksort}
output:
  pdf_document:
    latex_engine: xelatex
---

Divide y conquista es una técnica de diseño de algoritmos que permite resolver problemas complejos, partiéndolos repetidamente en problemas cada vez más pequeños, hasta llegar a problemas que son de trivial solución. Por ejemplo, consideremos la complejidad de ordenar una secuencia que contiene tan solo un elemento, o buscar un número en una secuencia que igualmente solo contiene un elemento. En el caso del ordenamiento, no hay nada que hacer, la secuencia de un elemento ya está en orden. En el caso de la búsqueda, la solución es trivial, hay que comparar el único elemento de la secuencia con el elemento buscado.

Aplicando esta técnica dividimos el problema en partes iguales, usualmente en mitades, cada una de estas mitades será ahora un subproblema. Tomemos por ejemplo el problema de ordenar una secuencia de $n$ números, si partimos la secuencia en dos obtendremos dos secuencias nuevas con aproximadamente $\frac{n}{2}$ elementos cada una. Ordenar una subsecuencia de estas es más sencillo que ordenar la secuencia completa simplemente porque contiene menos elementos, la mitad para ser precisos.

Posteriormente, resolvemos cada subproblema recursivamente aplicándoles a su vez divide y conquista. Una vez finalizada la llamada recursiva sobre cada subproblema tendremos dos subsecuencias debidamente ordenadas. Finalmente, juntamos las dos subsecuencias o soluciones parciales en una solución final.

Este último paso no necesariamente es trivial, por ejemplo si nuestras soluciones parciales son $[1, 3, 5]$ y $[2, 4, 6]$, la secuencia final no puede ser simplemente $[1, 3, 5, 2, 4, 6]$ pues no cumpliría con el orden requerido. El algoritmo de fusión de las soluciones parciales debe tener alguna lógica de selección para determinar que la secuencia final es $[1, 2, 3, 4, 5, 6]$. El supuesto del que partimos para que nuestro algoritmo divide y conquista tenga sentido, es que fusionar dos soluciones parciales es más sencillo que resolver el problema completo original; es decir, el hecho de que las subsoluciones ya estén en orden debería ser una ayuda para que el algoritmo que las junta en una solución final pueda actuar con eficiencia.

Por esta razón, la eficiencia de nuestro algoritmo divide y conquista dependerá de que la complejidad de juntar las soluciones parciales sea menor a la complejidad de solucionar cada subproblema.

## Búsqueda binaria ##

Consideremos el problema de buscar un número en una secuencia de números.

**Problema**. Buscar la posición de un número en una secuencia de números.  
  **Entradas**. Una secuencia de números $A = [a_1, a_2, \dots, a_n]$ de tamaño $n$ tal que $A$ es una permutación aleatoria y un número $e$ tal que $e \in A \lor e \notin A$.  
  **Salida**. Un número natural $i$ tal que $i \le n$ y $a_i = e$, o $-1$ cuando $e \notin A$.

Dado que $A$ se encuentra en orden aleatorio no nos queda más remedio que resolver este problema con un algoritmo de búsqueda lineal que recorra la secuencia elemento por elemento hasta encontrar al elemento buscado $e$. En el peor caso de que $e \notin A$ el algoritmo tendrá una cota superior de complejidad temporal $\mathcal{O}(n)$.

Sin embargo, si modificamos ligeramente el problema estableciendo que la secuencia está ordenada, podemos aplicar la técnica de divide y conquista para recursivamente reducir el espacio de búsqueda de tamaño $n$ a un espacio de búsqueda de tamaño $1$. Buscar a $e$ en un $A^\prime$ tal que $|A^\prime| = 1$ es trivial, basta con preguntar si $a_1 = e$, esta pregunta tiene complejidad temporal $\mathcal{O}(1)$.

Estudiemos entonces el algoritmo de búsqueda binaria.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de $n$ enteros $A = [a_0, a_1, \dots, a_{n-1}]$ tal que $a_0 \le a_1 \le \dots \le a_{n-1}$ y un número $e$ tal que $e \in A \lor e \notin A$}
    \KwOutput{Un número natural $i$ tal que $i < n$ y $a_i = e$, o $-1$ cuando $e \notin A$.}
  
    \BlankLine
    \caption{Búsqueda binaria}
    \SetAlgoVlined

    $inicio \assign 0$ \;
    $fin \assign n - 1$ \;

    \While{$inicio \le fin$} {
        $mitad \assign \left \lfloor{\frac{inicio + fin}{2}}\right \rfloor$ \;

        \If{$e = A[mitad]$} {
            \Return {$mitad$}
        } \ElseIf{$e < mitad$} {
            $fin \assign mitad - 1$ \;
        } \ElseIf{$e > mitad$} {
            $fin \assign mitad + 1$ \;
        }
    }

    \Return{$-1$} \;
\end{algorithm}

Este algoritmo trabaja siempre sobre un espacio de búsqueda delimitado por los punteros ${inicio}$ y ${fin}$. Al principio estos punteros se colocan respectivamente sobre el primer y último elemento de la secuencia. Buscamos nuestro elemento $e$ en la mitad del espacio de búsqueda. Con suerte $e$ estará en la ${mitad}$, pero sino es el caso, tendremos entonces dos posibilidades: $e$ puede ser mayor que el número que está en la mitad del espacio de búsqueda, o $e$ puede ser menor.

Cuando $e$ es mayor que el número en la mitad del espacio de búsqueda podemos concluir definitivamente que es imposible encontrar a $e$ en una posición anterior a la mitad ya que sabemos que la secuencia está ordenada ascendentemente. De esta forma podemos descartar todo el espacio de búsqueda a la izquierda de la mitad, corriendo al puntero ${inicio}$ a la derecha de ${mitad}$. Reducimos de esta forma el espacio de búsqueda en aproximadamente $\frac{n}{2}$ y repetimos el proceso.

De manera análoga, cuando $e$ es menor que el número a la mitad del espacio de búsqueda, podemos descartar todo el espacio de búsqueda a la derecha de la mitad, efectivamente disminuyendo a la mitad el espacio de búsqueda.

¿Cuál es la complejidad temporal de este algoritmo? El número de repeticiones del algoritmo depende del contenido de la secuencia $A$ y por tanto debemos hacer un análisis por casos.

### Peor y mejor caso ###

En el mejor de los casos, el número $e$ se encontrará en la mitad de la secuencia. En este caso, el algoritmo se detendrá en la primera iteración. El mejor caso tiene entonces una complejidad temporal de $\mathcal{O}(1)$.

$$
\therefore T_{mejor}(n) = \mathcal{O}(1)
$$

En el peor de los casos, el número $e$ no estará en la secuencia $A$. En este caso, tendremos que dividir a la mitad el espacio de búsqueda una cantidad determinada de veces hasta obtener un espacio de tamaño $1$ y determinar que el número $e$ no se encuentra en la lista.

¿Cuántas veces podemos dividir $n$ entre $2$ hasta obtener un espacio de tamaño $1$?

Observemos que después de la primera iteración tendremos un espacio de tamaño $\frac{n}{2}$. En la siguiente iteración tendremos

$$
\frac{\frac{n}{2}}{2} = \frac{\frac{n}{2}}{\frac{2}{1}} = \frac{n}{4}
$$

Y en la siguiente iteración

$$
\frac{\frac{n}{4}}{2} = \frac{n}{8}
$$

En términos generales, para la i-ésima iteración tenemos que

$i$      tamaño
-----    --------
`0`      $n$
`1`      $\frac{n}{2}$
`2`      $\frac{n}{4}$
`3`      $\frac{n}{8}$
$\vdots$ 
`i`      $\frac{n}{2^i}$

Podemos replantear nuestra pregunta ¿cuántas veces podemos dividir n entre 2 hasta obtener un espacio de tamaño 1?, como la siguiente ecuación

$$
\frac{n}{2^i} = 1
$$

Siendo $i$ la cantidad de veces que podemos dividir el espacio de búsqueda entre dos hasta obtener un espacio de búsqueda de tamaño $1$.

Si despejamos $i$ obtenemos

$$
\begin{aligned}
\frac{n}{2^i} &= 1 \\
n &= 2^i \\
i &= \log_2(n) \\
\end{aligned}
$$

Podemos, entonces, expresar el peor caso como

$$
\begin{aligned}
&T_{peor}(n) = \log_2(n) + 1 \\
\therefore &T_{peor}(n) = \mathcal{O}(\log n)
\end{aligned}
$$

### Caso promedio ###

Para estimar el caso promedio, primero establecemos algunos puntos de partida. Supongamos una distribución uniforme de los valores en la secuencia que constituye el espacio de búsqueda, es decir todas las posiciones son equiprobables. Supongamos también que $e \in A$.

Definimos la variable discreta aleatoria $X$ como el número de iteraciones necesarias para encontrar la posición de $e$ en $A$. Analicemos ahora las probabilidades para los distintos valores de $X$. 

Para encontrar $e$ en la primera iteración, tendría que estar en la mitad de la secuencia. Sólo hay una posición entre $n$ que cumple con esta condición, por tanto

$$
P(X = 1) = \frac{1}{n}
$$

Para encontrar $e$ en la segunda iteración hay dos posibilidades. Después de haber partido el espacio de búsqueda en la primera iteración, el elemento $e$ podría estar en la mitad del subespacio de búsqueda izquierdo, o podría estar en la mitad del subespacio derecho. Por tanto

$$
P(X = 2) = \frac{2}{n}
$$

Para encontrar $e$ en la tercera iteración hay cuatro posibilidades, la primera iteración parte el espacio de búsqueda en dos, la segunda iteración a su vez parte los dos subespacios en dos partes más cada uno. Tenemos entonces que

$$
P(X = 3) = \frac{4}{n}
$$

Observamos entonces un patrón que podemos generalizar para la i-ésima iteración de la siguiente manera

$$
P(X = i)= \frac{2^{i-1}}{n}
$$

Para estimar el caso promedio necesitamos determinar la expectativa para $X$. Recordemos además que el número máximo de iteraciones posibles antes de agotar el espacio de búsqueda es $\log_2 n$.

$$
\begin{aligned}
T_{prom}(n) &= E[X] \\
 &= \sum_{i=1}^{\log_{2}n} i \frac{2^{i-1}}{n} \\
 &= \frac{1}{n} \sum_{i=1}^{\log_{2}n} i 2^{i-1} \\
\end{aligned}
$$

Resolvamos la sumatoria antes de volver a $T_{prom}(n)$. Reemplazamos $\log_2 n$ con $k$.

$$
\begin{aligned}
\sum_{i=1}^{k} i 2^{i-1}
\end{aligned}
$$

Podemos cambiar los límites de la sumatoria para obtener la forma de la distribución geométrica.

$$
\begin{aligned}
\sum_{i=1}^{k} i 2^{i-1} = \sum_{i=0}^{k-1} (i + 1) 2^i
\end{aligned}
$$

Si distribuimos los términos

$$
\begin{aligned}
\sum_{i=0}^{k-1} (i + 1) 2^i &= \sum_{i=0}^{k-1} i 2^i + 2^i \\
 &= \sum_{i=0}^{k-1} i 2^i + \sum_{i=0}^{k-1} 2^i \\
\end{aligned}
$$

Ambas sumatorias son formas de series geométricas para las que ya conocemos los siguientes resultados

\begin{equation}
\sum_{i=0}^{n} c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1
\end{equation}

\begin{equation}
\sum_{i=0}^{n} i c^i = \frac{nc^{n+2}-(n+1)c^{n+1}+c}{(c-1)^2}, c \neq 1
\end{equation}

Reemplazando valores, tenemos que

$$
\begin{aligned}
\sum_{i=0}^{k-1} i 2^i &= \frac{(k-1)2^{(k-1)+2}-((k-1)+1)2^{(k-1)+1}+2}{(2-1)^2} \\
 &= (k-1) 2^{k+1} - k 2^k + 2 \\
 &= (k-1) 2^k 2 - k 2^k + 2
\end{aligned}
$$

Y también que

$$
\begin{aligned}
\sum_{i=0}^{k-1} 2^i &= \frac{2^{(k-1)+1}-1}{2-1} \\
 &= 2^k - 1 \\
\end{aligned}
$$

Si volvemos a nuestra expresión original en términos de $k$ tenemos entonces que

$$
\begin{aligned}
\sum_{i=1}^{k} i 2^{i-1} &= \sum_{i=0}^{k-1} i 2^i + \sum_{i=0}^{k-1} 2^i \\
 &= (k-1) 2^k 2 - k 2^k + 2 + 2^k - 1 \\
 &= (k-1) 2^k 2 - k 2^k + 2^k + 1 \\
 &= 2^k(2(k-1) - k + 1) + 1 \\
 &= 2^k(2k - 2 - k + 1) + 1 \\
 &= 2^k(k - 1) + 1 \\
\end{aligned}
$$

Si restauramos el valor de $k$ a $log_2 n$

$$
\begin{aligned}
\sum_{i=1}^{\log_{2}n} i 2^{i-1} &= 2^{\log_2 n}({\log_2 n} - 1) + 1\\
 &= n({\log_2 n} - 1) + 1\\
 &= n \log_2 n - n + 1\\
\end{aligned}
$$

Si retornamos a $T_{prom}(n)$

$$
\begin{aligned}
T_{prom}(n) &= \frac{1}{n} \sum_{i=1}^{\log_{2}n} i 2^{i-1} \\
 &= \frac{1}{n} \cdot (n \log_2 n - n + 1) \\
 &= \frac{1}{n} \cdot \mathcal{O}(n \log_2 n) \\
 &= \mathcal{O}(\frac{n \log_2 n}{n}) \\
 &= \mathcal{O}(\log_2 n) \\
 \therefore T_{prom}(n) &= \mathcal{O}(\log n) \\
\end{aligned}
$$

## Quicksort ##

Otro caso interesante de un algoritmo divide y conquista es el algoritmo de *quicksort* para resolver el problema de ordenamiento.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de $n$ enteros $A$ tal que $n \ge 1$ y $\forall i \in [0 \twodots (n-1)[$ se cumple que $a_i \le a_{i+1} \lor a_i \ge a_{i+1}$}
    \KwOutput{Una permutación de $A$, $A^\prime = [a_1, a_2, \dots, a_n]$ tal que $a_1 \le a_2 \le \dots \le a_n$}
  
    \BlankLine
    \caption{Quicksort}
    \SetAlgoVlined

    \If{$n \leq 1$} {
        \Return{$A$} \:
    }

    $pivote \assign A[n-1]$ \;
    $menores, mayores \assign \Particionar(A, pivote)$ \;

    $ordenada \assign \Quicksort(menores) \cup pivote \cup \Quicksort(mayores)$ \;

    \Return{$ordenada$} \;
\end{algorithm}

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de $n$ enteros $A$ y un número entero $pivote$}
    \KwOutput{Un par de secuencias $(m, M)$ donde $m \subseteq A$ y $\forall e \in m, e < pivote$; y $M \subseteq A$ y $\forall e \in M, e \geq pivote$}
  
    \BlankLine
    \caption{Particionar}
    \SetAlgoVlined

    $mayores \assign []$ \;
    $menores \assign []$ \;

    \For{$i \in [0, n-1[$} {
        \If{$A[i] < pivote$} {
            $menores \twoheadleftarrow A[i]$ \;
        } \Else {
            $mayores \twoheadleftarrow A[i]$ \;
        }
    }

    \Return{$(menores, mayores)$}
\end{algorithm}

Este algoritmo trabaja partiendo la secuencia a ordenar en dos partes con base en un valor de pivote tomado del contenido mismo de la secuencia (*Quicksort* línea 2). Dado que la secuencia es una permutación aleatoria el pivote puede ser cualquier elemento; particularmente en nuestro algoritmo siempre se toma el último elemento de la secuencia como pivote. La partición del problema en subproblemas se hace comparando cada elemento de la secuencia con el pivote (*Particionar* líneas 3-7), generando dos nuevas subsecuencias, una que contiene todos los elementos que son menores al pivote y otra que contiene todos los elementos que son mayores o iguales. Este proceso de subdivisión se repite hasta que ya no se pueda particionar más las secuencias; estas últimas subsecuencias(de tamaño $n \le 1$) ya estarán ordenadas por si mismas.

Cada subproblema es entonces tratado con el mismo algoritmo recursivamente, de manera tal que al final de la primera llamada tendremos el pivote, la subsecuencia de elementos menores al pivote ya ordenada, y la subsecuencia de elementos mayores o iguales al pivote también ordenada. El último paso es entonces volver a unir la secuencia colocando al pivote en el centro, a las menores a la izquierda del pivote y a las mayores a su derecha (*Quicksort* línea 3).