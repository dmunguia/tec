---
title: "Capítulo 5. Divide y conquista"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Marzo, 2019"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\inter}{inter}
output:
  pdf_document:
    latex_engine: xelatex
---

Divide y conquista es una técnica de diseño de algoritmos que permite resolver problemas complejos, partiéndolos repetidamente en problemas cada vez más pequeños, hasta llegar a problemas que son de trivial solución. Por ejemplo, consideremos la complejidad de ordenar una secuencia que contiene tan solo un elemento, o buscar un número en una secuencia que igualmente solo contiene un elemento.

Aplicando esta técnica dividimos el problema en partes iguales, usualmente en mitades, cada una de estas mitades será ahora un subproblema. Tomemos por ejemplo el problema de ordenar una secuencia de $n$ números, si partimos la secuencia en dos obtendremos dos secuencias nuevas con aproximadamente $\frac{n}{2}$ elementos cada una. Ordenar una subsecuencia de estas es más sencillo que ordenar la secuencia completa simplemente porque contiene menos elementos, la mitad para ser precisos.

Posteriormente, resolvemos cada subproblema recursivamente aplicándoles a su vez divide y conquista. Una vez finalizada la llamada recursiva sobre cada subproblema tendremos dos subsecuencias debidamente ordenadas. Finalmente, juntamos las dos subsecuencias o soluciones parciales en una solución final.

Este último paso no necesariamente es trivial, por ejemplo si nuestras soluciones parciales son $[1, 3, 5]$ y $[2, 4, 6]$, la secuencia final no puede ser simplemente $[1, 3, 5, 2, 4, 6]$ pues no cumpliría con el orden requerido. El algoritmo de fusión de las soluciones parciales debe tener alguna lógica de selección para determinar que la secuencia final es $[1, 2, 3, 4, 5, 6]$. El supuesto del que partimos para que nuestro algoritmo divide y conquista tenga sentido, es que fusionar dos soluciones parciales es más sencillo que resolver el problema completo original; es decir, el hecho de que las subsoluciones ya estén en orden debería ser una ayuda para que el algoritmo que las junta en una solución final pueda actuar con eficiencia.

Por esta razón, la eficiencia de nuestro algoritmo divide y conquista dependerá de que la complejidad de juntar las soluciones parciales sea menor a la complejidad de solucionar cada subproblema.

## Búsqueda binaria ##

Consideremos el problema de buscar un número en una secuencia de números.

**Problema**. Buscar la posición de un número en una secuencia de números.  
  **Entradas**. Una secuencia de números $A = [a_1, a_2, \dots, a_n]$ de tamaño $n$ tal que $A$ es una permutación aleatoria y un número $e$ tal que $e \in A \lor e \notin A$.  
  **Salida**. Un número natural $i$ tal que $i \le n$ y $a_i = e$, o $-1$ cuando $e \notin A$.

Dado que $A$ se encuentra en orden aleatorio no nos queda más remedio que resolver este problema con un algoritmo de búsqueda lineal que recorra la secuencia elemento por elemento hasta encontrar al elemento buscado $e$. En el peor caso de que $e \notin A$ el algoritmo tendrá una cota superior de complejidad temporal $\mathcal{O}(n)$.

Sin embargo, si modificamos ligeramente el problema estableciendo que la secuencia está ordenada, podemos aplicar la técnica de divide y conquista para recursivamente reducir el espacio de búsqueda de tamaño $n$ a un espacio de búsqueda de tamaño $1$. Buscar a $e$ en un $A^\prime$ tal que $|A^\prime| = 1$ es trivial, basta con preguntar si $a_1 = e$, esta pregunta tiene complejidad temporal $\mathcal{O}(1)$.

Estudiemos entonces el algoritmo de búsqueda binaria.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de $n$ enteros $A = [a_0, a_1, \dots, a_{n-1}]$ tal que $a_0 \le a_1 \le \dots \le a_{n-1}$ y un número $e$ tal que $e \in A \lor e \notin A$}
    \KwOutput{Un número natural $i$ tal que $i < n$ y $a_i = e$, o $-1$ cuando $e \notin A$.}
  
    \BlankLine
    \caption{Búsqueda binaria}
    \SetAlgoVlined

    $inicio \assign 0$ \;
    $fin \assign n - 1$ \;

    \While{$inicio \le fin$} {
        $mitad \assign \left \lfloor{\frac{inicio + fin}{2}}\right \rfloor$ \;

        \If{$e = A[mitad]$} {
            \Return {$mitad$}
        } \ElseIf{$e < mitad$} {
            $fin \assign mitad - 1$ \;
        } \ElseIf{$e > mitad$} {
            $fin \assign mitad + 1$ \;
        }
    }

    \Return{$-1$} \;
\end{algorithm}

Este algoritmo trabaja siempre sobre un espacio de búsqueda delimitado por los punteros ${inicio}$ y ${fin}$. Al principio estos punteros se colocan respectivamente sobre el primer y último elemento de la secuencia. Buscamos nuestro elemento $e$ en la mitad del espacio de búsqueda. Con suerte $e$ estará en la ${mitad}$, sino es el caso, tendremos entonces dos posibilidades: $e$ puede ser mayor que el número que está en la mitad del espacio de búsqueda, o $e$ puede ser menor.

Cuando $e$ es mayor que el número en la mitad del espacio de búsqueda podemos concluir definitivamente que es imposible encontrar a $e$ en una posición anterior a la mitad ya que sabemos que la secuencia está ordenada ascendentemente. De esta forma podemos descartar todo el espacio de búsqueda a la izquierda de la mitad, corriendo al puntero ${inicio}$ a la derecha de ${mitad}$. Reducimos de esta forma el espacio de búsqueda en $\frac{n}{2}$ y repetimos el proceso.

De manera análoga, cuando $e$ es menor que el número a la mitad del espacio de búsqueda, podemos descartar todo el espacio de búsqueda a la derecha de la mitad, efectivamente disminuyendo a la mitad el espacio de búsqueda.

¿Cuál es la complejidad temporal de este algoritmo? El número de repeticiones del algoritmo depende del contenido de la secuencia $A$ y por tanto debemos hacer un análisis por casos.

En el mejor de los casos, el número $e$ se encontrará en la mitad de la secuencia. En este caso, el algoritmo se detendrá en la primera iteración. El mejor caso tiene entonces una complejidad temporal de $\mathcal{O}(1)$.

En el peor de los casos, el número $e$ no estará en la secuencia $A$. En este caso, tendremos que dividir a la mitad el espacio de búsqueda una cantidad determinada de veces hasta obtener un espacio de tamaño $1$ y determinar que el número $e$ no se encuentra en la lista.

¿Cuántas veces podemos dividir $n$ entre $2$ hasta obtener un espacio de tamaño $1$?

Observemos que después de la primera iteración tendremos un espacio de tamaño $\frac{n}{2}$. En la siguiente iteración tendremos

$$
\frac{\frac{n}{2}}{2} = \frac{\frac{n}{2}}{\frac{2}{1}} = \frac{n}{4}
$$

Y en la siguiente iteración

$$
\frac{\frac{n}{4}}{2} = \frac{n}{8}
$$

En términos generales

$i$      tamaño
-----    --------
`0`      $n$
`1`      $\frac{n}{2}$
`2`      $\frac{n}{4}$
`3`      $\frac{n}{8}$
$\vdots$ 
`i`      $\frac{n}{2^i}$

Podemos replantear nuestra pregunta con la ecuación

$$
\frac{n}{2^i} = 1
$$

Siendo $i$ la cantidad de veces que podemos dividir el espacio de búsqueda entre dos hasta obtener un espacio de búsqueda de tamaño $1$.

Si despejamos $i$ obtenemos

$$
\begin{aligned}
\frac{n}{2^i} &= 1 \\
n &= 2^i \\
i &= \log_2(n) \\
\end{aligned}
$$

Podemos, entonces, expresar el peor caso como

$$
\begin{aligned}
&T_{peor}(n) = \log_2(n) + 1 \\
\therefore &T_{peor}(n) = \mathcal{O}(\log n)
\end{aligned}
$$
