---
title: "Capítulo 9. Algoritmos voraces"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Marzo, 2019"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \usepackage{comment}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\ordenar}{ordenar}
    - \DeclareMathOperator{\bits}{bits}
output:
  pdf_document:
    latex_engine: xelatex
---

En este capítulo estudiaremos una técnica heurística que podemos aplicar a problemas de optimización en búsqueda combinatoria, los algoritmos voraces. Para algunos problemas, el uso de esta técnica permite encontrar soluciones óptimas a través de un proceso progresivo en el que en cada paso del algoritmo se decide utilizar la opción inmediata mejor entre la serie de opciones posibles, sin consideraciones sobre cómo esta decisión impacta el resultado global del algoritmo. La heurística implica asumir que escoger la mejor opción disponible en cada paso nos llevará a finalizar con la mejor solución final posible para el problema.

Para introducir el uso de esta técnica, consideremos el problema de calendarizar el uso de un único recurso disponible de forma tal que se obtenga el máximo provecho posible.

## Selección de actividades ##

En un centro universitario tenemos a disposición de la comunidad un auditorio en el cuál se pueden llevar a cabo actividades. El auditorio es muy solicitado, y por tanto se desea acomodar la mayor cantidad de actividades por día como sea posible.

Para calendarizar el uso del auditorio en un día particular se reciben las solicitudes de actividades a realizar indicando para cada actividad su hora de inicio y hora de finalización. Puesto que cada solicitud se recibe individualmente, y no hay un requisito de coordinación entre los solicitantes, es muy probable que hayan choques de horario entre algunas de las actividades solicitadas.

El problema a resolver es entonces seleccionar la mayor cantidad de actividades posibles para desarrollarse en el auditorio en un mismo día sin que haya choque de horarios.

Podemos estructurar las solicitudes de actividades para un día en una tabla que contiene para cada actividad su hora de inicio y hora de finalización, por ejemplo de la siguiente manera:

actividad   1   2   3   4   5   6   7   8   9  10  11
--------   --  --  --  --  --  --  --  --  --  --  --
inicio      8   9  10  11  11  13  13  14  16  16  20
fin        14  12  21  13  16  15  17  18  19  20  22

En esta tabla podemos observar por ejemplo que las actividades 1 y 2 tienen choque de horario, puesto que la actividad 1 termina a las 14 horas (2 pm) mientras que la actividad 2 inicia a las 9 horas (9 am). Por otro lado, la actividad 6 no choca con la actividad 9, porque la 6 termina a las 15 horas (3 pm) mientras que la 9 inicia a las 16 horas (4 pm).

Cuando dos actividades no tienen choque de horario decimos que son compatibles, y cuando sus horarios chocan decimos que son incompatibles. Podemos expresar esta relación de la siguiente manera:

(@) **Definición**. Dos actividades $i$ y $j$ son **compatibles** entre si cuando $inicio_i \geq fin_j \lor inicio_j \geq fin_i$.

Expresamos entonces el problema computacionalmente de la siguiente manera.

(@) **Problema**. Seleccionar la mayor cantidad de actividades compatibles entre si.  
  **Entradas**. Un conjunto de actividades $Act = \{1, 2, 3, \dots n\}$ y una secuencia de pares de la forma $(inicio, fin)$ correspondiente a los horarios para cada actividad $H = [(i_1, f_1), (i_2, f_2), \dots, (i_n, f_n)]$.  
  **Salida**. El subconjunto $Sel \subseteq Act$ de mayor cardinalidad tal que $\forall i, j \in Sel, i_i \geq f_j \lor i_j \leq f_i$.  

Para resolver este problema aplicamos la heurística voraz de seleccionar siempre del conjunto de actividades posibles aquella que (1) finalice primero y (2) que sea compatible con la última actividad seleccionada.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de actividades $Act = [(a_1, i_1, f_1), (a_2, i_2, f_2), \dots, (a_n, i_n, f_n)]$.}
    \KwOutput{El subconjunto $Sel \subseteq Act$ de mayor cardinalidad tal que $\forall i, j \in Sel, i_i \geq f_j \lor i_j \leq f_i$.}
  
    \BlankLine
    \caption{Seleccionar la mayor cantidad de actividades compatibles entre si.}
    \SetAlgoVlined
    
    $\ordenar(Act)$ ordenar actividades ascendentemente por hora de finalización \;
    $n \assign |Act|$ \;
    $Sel \assign \{Act[0]\}$ \;
    $j \assign 0$ \;

    \For{$i \in [1 \twodots n[$} {
        \If{$Act[i].i \geq Act[j].f$} {
            $Sel \assign Sel \cup \{Act[i]\}$ \;
            $ j \assign i $
        }
    }

    \Return{$Sel$} \;
\end{algorithm}

El primer paso del algoritmo es ordenar la lista de actividades ascendentemente por hora de finalización (línea 1). Esto nos permitirá ir tomando siempre de la secuencia las actividades que finalicen más pronto. El índice $j$ nos permitirá mantener la referencia a la última actividad agregada a $Sel$ (líneas 4 y 8).

Una vez agregada la primera actividad de la secuencia a $Sel$, es decir la que finaliza más pronto en el día (línea 3), recorremos el resto de la secuencia, preguntando en cada iteración si la actividad actual es compatible con la última actividad agregada a $Sel$ (línea 6), si es compatible la agregamos a $Sel$.

La complejidad temporal de este algoritmo debe tomar en cuenta el tiempo necesario para el ordenamiento así como el recorrido lineal que se realiza sobre la secuencia de actividades $Act$. Para el ordenamiento podemos esperar como mejor opción una complejidad temporal de $\mathcal{O}(n \log n)$.

$$
\begin{aligned}
T(n) = \mathcal{O}(n \log n) + \mathcal{O}(n) = \mathcal{O}(n \log n)
\end{aligned}
$$

### Algoritmos voraces ###

Para decidir si podemos resolver un problema con un algoritmo voraz debemos verificar que el problema cumpla con dos características particulares: subestructura óptima y opción voraz.

La característica de **subestructura óptima** es la misma que necesitamos para resolver problemas con programación dinámica. Recordamos del capítulo anterior que un problema tiene subestructura óptima cuando su solución óptima contiene a su vez la solución óptima para sus subproblemas.

Por otro lado, la característica de **opción voraz** se refiere a que podemos llegar a una solución óptima global escogiendo la opción óptima local en cada repetición del algoritmo. A diferencia de la técnica de programación dinámica, la opción voraz no implica que tenemos que resolver el subproblema para saber cuál es su mejor solución, al contrario simplemente se escoge la opción que aparentemente es la mejor de acuerdo con algún criterio definido para el problema particular en cuestión. Por ejemplo, en el caso de nuestro algoritmo para selección de actividades la opción voraz es escoger en cada paso la actividad que termine más temprano, esto no nos lleva a investigar y resolver un subproblema, simplemente es la mejor opción aparente en cada iteración del algoritmo.

### Correctitud ###

¿Cómo podemos tener certeza de que la heurística de opción voraz que utiliza nuestro algoritmo de selección de actividades siempre nos va a ayudar a generar la solución óptima?

Sabemos que la heurística voraz no garantiza encontrar la solución óptima para cualquier problema, sino que es necesario que presente subestructura óptima y opción voraz.

Podemos responder a la pregunta con una demostración formal de la siguiente manera.

(@) **Teorema**. El problema de selección de actividades se puede resolver con un algoritmo voraz que tome siempre la próxima actividad compatible que finalice más pronto.

#### Demostración ####

Para demostrar que nuestro algoritmo voraz garantiza la solución óptima al problema de selección de actividades debemos demostrar que tiene subestructura óptima y que la opción voraz produce una solución óptima.

*Opción voraz*. Supongamos que $S = [a_1, a_2, \dots, a_n]$ es una solución óptima al problema de selección de actividades para un conjunto de actividades $A$ tal que $a_1$ **no** es la actividad que finaliza más temprano.

Puesto que $S$ es una solución óptima sabemos que:

* $n = |S|$ es el número máximo de actividades posible de acomodar.

* $fin(a_1) \leq inicio(a_2)$, $fin(a_2) \leq inicio(a_3)$ y así sucesivamente para el resto de actividades en la secuencia $S$.

* En general, $\underset{i<j}{\forall i,j} \in [1 \twodots n], fin(a_i) \leq inicio(a_j)$.

Si $a_t$ es la actividad que finaliza más temprano, entonces se cumple que $fin(a_t) \leq fin(a_1)$. Dado que $fin(a_1) \leq inicio(a_2)$ entonces se cumple por transitividad que $fin(a_t) \leq inicio(a_2)$.

Entonces podemos reemplazar $a_1$ por $a_t$ en $S$ manteniendo la compatibilidad entre las actividades. Si aplicamos dicho reemplazo, obtenemos entonces una secuencia $S_v = [a_t, a_2, \dots, a_n]$ tal que $|S_v|=n$. Por tanto, $S_v$ es también una solución óptima.

$\therefore$ La selección de la actividad que finaliza más temprano como opción voraz genera una solución óptima.

*Subestructura óptima*. Consideremos ahora el subproblema $A^{\prime} = A - \{a_t\}$. Podemos demostrar que dado que $S_v$ es una solución óptima al problema $A$, entonces $S_{v}^{\prime} = S_v - [a_t] = [a_2, \dots, a_n]$ es una solución óptima al subproblema $A^{\prime}$.

Sabemos que:

* $|S_v| = n$

* $|S_{v}^{\prime}| = (n - 1)$

Para demostrar por contradicción, supongamos que $S_{v}^{\prime}$ no es la solución óptima al subproblema $A^{\prime}$ y que más bien existe una solución óptima $T^{\prime}$ tal que $|T^{\prime}| > |S_{v}^{\prime}|$, en otras palabras $T^{\prime}$ logra acomodar más actividades que $S_{v}^{\prime}$.

$$
|T^{\prime}| > |S_{v}^{\prime}| \implies |T^{\prime}| > (n - 1)
$$

Si este es el caso, entonces podemos construir a $T = T^{\prime} + [a_t]$ como una solución óptima al problema $A$ (puesto que ya demostramos que el problema presenta opción voraz). Según la definición de $T$, tendríamos que $|T| = |T^{\prime} + 1|$. Con base en este hecho hacemos la siguiente derivación.

$$
\begin{aligned}
|T^{\prime}| &> (n - 1) \\
|T^{\prime}| + 1 &> (n - 1) + 1 \\
|T^{\prime}| + 1 &> n \\
|T| &> n \\
|T| &> |S_v| \\
\end{aligned}
$$

Este resultado implica entonces que $S_v$ no es una solución óptima al problema $A$ y caemos en contradicción con nuestra premisa. No puede haber, por lo tanto, una solución mejor que $S_{v}^{\prime}$ para el problema $A^{\prime}$.

$\therefore$ El problema de selección de actividades tiene subestructura óptima.

$\blacksquare$

## Árboles de Huffman ##

Cambiemos nuestra atención ahora a un problema distinto que también puede resolverse con un algoritmo voraz. El problema a resolver es el de comprimir el tamaño en bytes, o huella de almacenamiento, de un archivo de texto.

Supongamos que tenemos un archivo de texto que contiene una secuencia de 100,000 caracteres codificados en ASCII (8 bits por caracter). Este archivo tendría una huella de almacenamiento de 800,000 bits entonces.

El problema de compresión es lograr almacenar la misma información contenida en nuestro archivo de texto con una huella menor a 800,000 bits.

Supongamos ahora que analizamos el contenido de nuestro archivo de texto para registrar la frecuencia de cada uno de los caracteres que aparecen en el texto. Resumimos los resultados en la siguiente tabla.

              a    b    c    d    e    f
----------  ---  ---  ---  ---  ---  ---
frecuencia   45   13   12   16    9    5

La tabla nos muestra que el texto del archivo está compuesto por un alfabeto de únicamente seis caracteres `'a'`, `'b'`, `'c'`, `'d'`, `'e'` y `'f'`. Los números de la tabla representan miles de aparaciones, es decir el caracter `'a'` aparece 45,000 veces en el archivo, y el caracter `'e'` aparece 9,000 veces. Observamos que la suma de las frecuencias corresponde a los 100,000 caracteres de nuestro archivo.

Puesto que solo tenemos seis caracteres en nuestro archivo, en lugar de representar cada caracter con una hilera de 8 bits (1 byte) podemos representarlos con una hilera de tan solo 3 bits.

               a    b    c    d    e    f
----------   ---  ---  ---  ---  ---  ---
frecuencia    45   13   12   16    9    5
código fijo  000  001  010  011  100  101

Si logramos aplicar esta representación, obtendremos una huella de almacenamiento de

$$
100,000 \times 3 \bits = 300,000 \bits
$$

Esta nueva representación representa una 62.5% en la huella de almacenamiento original.

Los árboles de Huffman nos permiten ir un paso más allá, asignando una codificación de tamaño variable que permite una reducción aún mayor de la huella de almacenamiento del archivo. Para nuestra tabla de frecuencia esta codificación variable sería la siguiente.

                   a    b    c    d     e     f
----------       ---  ---  ---  ---  ----  ----
frecuencia        45   13   12   16     9     5
código variable    0  101  100  111  1101  1100

Notamos que en esta nueva codificación hay una relación inversa entre la longitud del código y la frecuencia del caracter. Los caracteres más frecuentes tienen códigos más cortos mientras que los menos frecuentes se codifican con más bits. Esta codificación variable produce una huella de almacenamiento para nuestro archivo de

$$
(45 \times 1 \bits + 13 \times 3 \bits + 12 \times 3 \bits + 16 \times 3 \bits + 9 \times 4 \bits + 5 \times 4 \bits) \times 1000 = 224,000 \bits
$$

La nueva huella de 224,000 bits representa una reducción de 72% con respecto a la original.

Utilizando esta codificación podemos almacenar nuestro archivo de manera comprimida como una hilera de 224,000 bits sin prestar atención especial a la división típica en bytes (grupos 8 bits).

Para poder leer el contenido del archivo debemos descomprimirlo, es decir debemos reinterpretar la hilera de 224,000 bits de acuerdo con la codificación variable, para lograr esta reinterpretación utilizamos un árbol de Huffman como llave para descifrar los códigos.

Si nuestro archivo contiene la hilera `"bebacafe"` esta será representada con la siguiente hilera de bits.

$$
101110110101000110011010
$$

Para mapear la hilera de bits de vuelta a nuestra hilera `"bebacafe"` utilizamos el árbol de la figura $\ref{fig:huffman}$. Aún no nos preocuparemos por entender cómo se construye este árbol, por ahora nos enfocaremos en cómo podemos utilizarlo para descomprimir una hilera de bits.

![Árbol de Huffman\label{fig:huffman}](plots/huffman.png){ width=350px }

Para descomprimir la hilera la recorremos bit por bit de izquierda a derecha, al mismo tiempo que recorremos la hilera de bits también recorremos el árbol partiendo desde la raíz. Cada bit que leamos de la hilera nos va a indicar cuál de las ramas del nodo actual debemos seguir. Repetimos este recorrido hasta llegar a una hoja del árbol, en este punto hemos encontrado el primer código, este nodo hoja también nos indica a qué letra corresponde dicho código. A partir de este punto volvemos a iniciar el recorrido del árbol desde la raíz. Continuamos repitiendo estos recorridos hasta que lleguemos al final de la hilera de bits.

![Primer código\label{fig:huffman_rec_1}](plots/huffman_rec_1.png){ width=350px }

La figura $\ref{fig:huffman_rec_1}$ ilustra el recorrido para identificar el primer código en nuestra hilera de bits. Mientras que la figura $\ref{fig:huffman_rec_2}$ muestra el recorrido para identificar el segundo código.

![Segundo código\label{fig:huffman_rec_2}](plots/huffman_rec_2.png){ width=350px }

Ahora que comprendemos la utilidad del árbol para descomprimir el archivo, podemos enfocarnos en entender cómo se construye. Este es el problema que resolveremos con un algoritmo voraz.

Para construir el árbol tomamos como insumo la tabla de frecuencias de caracteres. Para cada caracter construimos un nodo que contiene el caracter  y su correspondiente frecuencia e insertamos este en una cola de prioridad ordenada por frecuencia. Esto quiere decir que en la cabeza de la cola siempre estará el nodo con menor frecuencia.

![Construyendo el árbol\label{fig:huffman_greedy}](plots/huffman_greedy.png){ width=350px }

La figura $\ref{fig:huffman_greedy}$ muestra en el punto 1 la configuración inicial de la cola de prioridad. En este punto inicia el recorrido sobre la cola, el cual repetiremos hasta que quede solo un nodo en la cola, este último nodo será la raíz del árbol. A continuación detallamos las operaciones en cada iteración:

1. Creamos un nuevo nodo $N$.
2. Extraemos un nodo $A$ de la cola y lo asociamos con la rama izquierda (0) de $N$.
3. Extraemos otro nodo $B$ de la cola y lo asociamos con la rama derecha (1) de $N$.
4. Asignamos a $N$ una frecuencia igual a la suma de las frecuencias de $A$ y $B$.
5. Insertamos $N$ en la cola.
6. Volvemos al punto 1.

La opción voraz de este algoritmo viene dada por el hecho de que siempre tomaremos los nodos con menor frecuencia. Esto implica que conforme avance el algoritmo quedarán nodos cada vez con frecuencias mayores, dejando para el final el nodo con mayor frecuencia. La construcción de abajo-hacia-arriba del árbol provoca que los nodos con menores frecuencias queden en los niveles más profundos del árbol; relacionado con esto observamos que la distancia del recorrido en el árbol desde la raíz hasta un nodo hoja cualquiera determina la longitud del código asociado con el caracter en la hoja. Por tanto, los nodos con menores frecuencias tendrán códigos más largos, mientras que los nodos con mayores frecuencias tendrán códigos más cortos.

Formalmente planteamos el problema computacional y el algoritmo que lo soluciona de la siguiente manera.

(@) **Problema**. Construir un árbol de Huffman.  
  **Entradas**. Una secuencia de pares $F = [(c_1, f_1), (c_2, f_2), \dots, (c_n, f_n)]$ representando la tabla de caracteres y sus correspondientes frecuencias.  
  **Salida**. Un árbol de Huffman que contiene los datos de $F$.  

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de pares $F = [(c_1, f_1), (c_2, f_2), \dots, (c_n, f_n)]$ representando la tabla de caracteres y sus correspondientes frecuencias.}
    \KwOutput{Un árbol de Huffman que contiene los datos de $F$.}
  
    \BlankLine
    \caption{Construir un árbol de Huffman}
    \SetAlgoVlined

    $n \assign |F|$ \;
    $Q \assign $ nueva cola de prioridad \;

    \For{$par \in F$} {
        $N \assign $ nuevo nodo \;
        $N.f \assign par.f$ \;
        $N.c \assign par.c$ \;
        $N \twoheadrightarrow Q$ \;
    }

    \While{$|Q| > 1$} {
        $N \assign $ nuevo nodo \;
        $A \twoheadleftarrow Q$ \;
        $B \twoheadleftarrow Q$ \;

        $N.izq \assign A$ \;
        $N.der \assign B$ \;
        $N.f \assign A.f + B.f$ \;

        $N \twoheadrightarrow Q$ \;
    }
    
    $árbol \twoheadleftarrow Q$ \;
    \Return{$árbol$} \;
\end{algorithm}

El análisis de comlejidad temporal de este algoritmo depende de la implementación de cola de prioridad que utilicemos. El algoritmo tiene dos ciclos, uno para llenar la cola y otro para construir el árbol.

Algunas colas presentan complejidad temporal $\Theta(1)$ para la operación de insertar mientras que otras presentan complejidad temporal $\mathcal{O}(\log n)$ para la misma operación. De esta forma, escogiendo la cola más eficiente, el ciclo de llenado de la cola (líneas 3-7) podría presentar complejidad temporal de $\mathcal{O}(n)$, puesto que en cada iteración del recorrido lineal realizaría $\mathcal{O}(1)$ operaciones. Mientras que escogiendo la cola menos eficiente en inserciones, tendríamos una complejidad temporal de $\mathcal{O}(n) \cdot \mathcal{O}(\log n) = \mathcal{O}(n \log n)$.

Por otro lado, la operación de eliminar de la cola o desencolar, es al menos $\mathcal{O}(\log n)$ para todas las colas. Por tanto, para el ciclo de construcción del árbol (líneas 8-15) tenemos un recorrido lineal que desencola en cada iteración, entonces podemos esperar una complejidad temporal de $\mathcal{O}(n \log n)$.

## Síntesis ##

Los algoritmos voraces tienden a ser más eficientes que otros algoritmos que resuelven los mismos problemas de búsqueda combinatoria aplicando otras técnicas (por ejemplo con programación dinámica o *backtracking*). Podemos afirmar esto con cierta confianza porque escoger la opción voraz tiende a ser una operación más simple —escoger el mínimo o el máximo local— que desarrollar una búsqueda exhaustiva sobre todo el espacio disponible.

Sin embargo, la complejidad en el uso de esta técnica yace en la necesidad de construir una demostración formal sobre las cualidades del problema en cuestión: subestructura óptima y opción voraz.

## Ejercicios ##

1. Resuelva el problema de selección de actividades utilizando (a) una búsqueda ingenua, (b) programación dinámica.

2. Retomando el ejercicio #3 del capítulo 8. Considere ahora la posibilidad de tomar partes de los paquetes de ayuda, es decir al cargar el camión no necesariamente tengo que cargar el volúmen completo de cada paquete. Proponga un algoritmo voraz que resuelva esta nueva versión del problema.

3. Una persona necesita conducir su automóvil de Limón a Liberia. Sabemos que el tanque del automóvil rinde $n$ kilómetros. Tenemos un mapa que indica las distancias entre las diferentes estaciones de combustible sobre la ruta a tomar. La persona quiere hacer la menor cantidad posible de paradas para cargar combustible. (a) Demuestre que este problema se puede resolver con un algoritmo voraz. (b) Diseñe un algoritmo voraz que resuelva este problema.

\begin{comment}
CLRS 17.2-4 https://sites.math.rutgers.edu/~ajl213/CLRS/Ch16.pdf
\end{comment}

4. Queremos cambiar un monto $n$ de colones a monedas utilizando la mínima cantidad posible de monedas. Las denominaciones de las monedas son 500, 100, 50, 25, 10, 5 y 1. (a) Diseñe un algoritmo voraz que resuelva el problema. (b) Demuestre que su algoritmo siempre va a producir una solución óptima.

\begin{comment}
CLRS 17-1 https://sites.math.rutgers.edu/~ajl213/CLRS/Ch16.pdf
\end{comment}

## Referencias ##

Cormen T., Leiserson C., Rivest R. y Stein C. (2009) Introduction to Algorithms (2da ed.). MIT Press.
