---
title: "Capítulo 12. Algoritmos de compresión"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Diciembre, 2020"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \usepackage{comment}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\ordenar}{ordenar}
    - \DeclareMathOperator{\bits}{bits}
    - \DeclareMathOperator{\contar}{contar}
    - \DeclareMathOperator{\Hilo}{Hilo}
    - \DeclareMathOperator{\start}{start}
    - \DeclareMathOperator{\join}{join}
    - \DeclareMathOperator{\adquirir}{adquirir}
    - \DeclareMathOperator{\liberar}{liberar}
    - \DeclareMathOperator{\Candado}{Candado}
    - \DeclareMathOperator{\probar}{probar}
    - \DeclareMathOperator{\copiar}{copiar}
    - \DeclareMathOperator{\pop}{pop}
output:
  pdf_document:
    latex_engine: xelatex
---

Los algoritmos de compresión tienen como objetivo transformar una secuencia de datos de una codificación original a una codificación nueva que requiera de menos bits para representar los mismos datos, o aproximadamente los mismos datos. Esta nueva codificación va a permitir almacenar o transmitir datos con una menor cantidad de bytes.

## Árboles de Huffman ##

Supongamos que tenemos un archivo de texto que contiene una secuencia de 100,000 caracteres codificados en ASCII. La codificación ASCII utiliza 8 bits por cada caracter. Nuestro archivo tiene entonces una huella de almacenamiento de 800,000 bits.

El problema de compresión es lograr almacenar la misma información contenida en nuestro archivo de texto con una huella menor a 800,000 bits.

Supongamos ahora que analizamos el contenido de nuestro archivo de texto para registrar la frecuencia de cada uno de los caracteres que aparecen en el texto. Resumimos los resultados en la siguiente tabla.

              a    b    c    d    e    f
----------  ---  ---  ---  ---  ---  ---
frecuencia   45   13   12   16    9    5

La tabla nos muestra que el texto del archivo está compuesto por un alfabeto de únicamente seis caracteres `'a'`, `'b'`, `'c'`, `'d'`, `'e'` y `'f'`. Los números de la tabla representan miles de aparaciones, es decir el caracter `'a'` aparece 45,000 veces en el archivo, y el caracter `'e'` aparece 9,000 veces. Observamos que la suma de las frecuencias corresponde a los 100,000 caracteres de nuestro archivo.

Puesto que solo tenemos seis caracteres en nuestro archivo, en lugar de representar cada caracter con una hilera de 8 bits (1 byte) podemos representarlos con una hilera de tan solo 3 bits.

               a    b    c    d    e    f
----------   ---  ---  ---  ---  ---  ---
frecuencia    45   13   12   16    9    5
código fijo  000  001  010  011  100  101

Si logramos aplicar esta representación, obtendremos una huella de almacenamiento de

$$
100,000 \times 3 \bits = 300,000 \bits
$$

Esta nueva representación representa una 62.5% en la huella de almacenamiento original.

Los árboles de Huffman nos permiten ir un paso más allá, asignando una codificación de tamaño variable que permite una reducción aún mayor de la huella de almacenamiento del archivo. Para nuestra tabla de frecuencia esta codificación variable sería la siguiente.

                   a    b    c    d     e     f
----------       ---  ---  ---  ---  ----  ----
frecuencia        45   13   12   16     9     5
código variable    0  101  100  111  1101  1100

Notamos que en esta nueva codificación hay una relación inversa entre la longitud del código y la frecuencia del caracter. Los caracteres más frecuentes tienen códigos más cortos mientras que los menos frecuentes se codifican con más bits. Esta codificación variable produce una huella de almacenamiento para nuestro archivo de

$$
(45 \times 1 \bits + 13 \times 3 \bits + 12 \times 3 \bits + 16 \times 3 \bits + 9 \times 4 \bits + 5 \times 4 \bits) \times 1000 = 224,000 \bits
$$

La nueva huella de 224,000 bits representa una reducción de 72% con respecto a la original.

Utilizando esta codificación podemos almacenar nuestro archivo de manera comprimida como una hilera de 224,000 bits sin prestar atención especial a la división típica en bytes (grupos 8 bits).

Para poder leer el contenido del archivo debemos descomprimirlo, es decir debemos reinterpretar la hilera de 224,000 bits de acuerdo con la codificación variable, para lograr esta reinterpretación utilizamos un árbol de Huffman como llave para descifrar los códigos.

Si nuestro archivo contiene la hilera `"bebacafe"` esta será representada con la siguiente hilera de bits.

$$
101110110101000110011010
$$

Para mapear la hilera de bits de vuelta a nuestra hilera `"bebacafe"` utilizamos el árbol de la figura $\ref{fig:huffman}$. Aún no nos preocuparemos por entender cómo se construye este árbol, por ahora nos enfocaremos en cómo podemos utilizarlo para descomprimir una hilera de bits.

![Árbol de Huffman\label{fig:huffman}](plots/huffman.png){ width=350px }

Para descomprimir la hilera la recorremos bit por bit de izquierda a derecha, al mismo tiempo que recorremos la hilera de bits también recorremos el árbol partiendo desde la raíz. Cada bit que leamos de la hilera nos va a indicar cuál de las ramas del nodo actual debemos seguir. Repetimos este recorrido hasta llegar a una hoja del árbol, en este punto hemos encontrado el primer código, este nodo hoja también nos indica a qué letra corresponde dicho código. A partir de este punto volvemos a iniciar el recorrido del árbol desde la raíz. Continuamos repitiendo estos recorridos hasta que lleguemos al final de la hilera de bits.

![Primer código\label{fig:huffman_rec_1}](plots/huffman_rec_1.png){ width=350px }

La figura $\ref{fig:huffman_rec_1}$ ilustra el recorrido para identificar el primer código en nuestra hilera de bits. Mientras que la figura $\ref{fig:huffman_rec_2}$ muestra el recorrido para identificar el segundo código.

![Segundo código\label{fig:huffman_rec_2}](plots/huffman_rec_2.png){ width=350px }

Ahora que comprendemos la utilidad del árbol para descomprimir el archivo, podemos enfocarnos en entender cómo se construye. Este es el problema que resolveremos con un algoritmo voraz.

Para construir el árbol tomamos como insumo la tabla de frecuencias de caracteres. Para cada caracter construimos un nodo que contiene el caracter  y su correspondiente frecuencia e insertamos este en una cola de prioridad ordenada por frecuencia. Esto quiere decir que en la cabeza de la cola siempre estará el nodo con menor frecuencia.

![Construyendo el árbol\label{fig:huffman_greedy}](plots/huffman_greedy.png){ width=350px }

La figura $\ref{fig:huffman_greedy}$ muestra en el punto 1 la configuración inicial de la cola de prioridad. En este punto inicia el recorrido sobre la cola, el cual repetiremos hasta que quede solo un nodo en la cola, este último nodo será la raíz del árbol. A continuación detallamos las operaciones en cada iteración:

1. Creamos un nuevo nodo $N$.
2. Extraemos un nodo $A$ de la cola y lo asociamos con la rama izquierda (0) de $N$.
3. Extraemos otro nodo $B$ de la cola y lo asociamos con la rama derecha (1) de $N$.
4. Asignamos a $N$ una frecuencia igual a la suma de las frecuencias de $A$ y $B$.
5. Insertamos $N$ en la cola.
6. Volvemos al punto 1.

La opción voraz de este algoritmo viene dada por el hecho de que siempre tomaremos los nodos con menor frecuencia. Esto implica que conforme avance el algoritmo quedarán nodos cada vez con frecuencias mayores, dejando para el final el nodo con mayor frecuencia. La construcción de abajo-hacia-arriba del árbol provoca que los nodos con menores frecuencias queden en los niveles más profundos del árbol; relacionado con esto observamos que la distancia del recorrido en el árbol desde la raíz hasta un nodo hoja cualquiera determina la longitud del código asociado con el caracter en la hoja. Por tanto, los nodos con menores frecuencias tendrán códigos más largos, mientras que los nodos con mayores frecuencias tendrán códigos más cortos.

Formalmente planteamos el problema computacional y el algoritmo que lo soluciona de la siguiente manera.

(@) **Problema**. Construir un árbol de Huffman.  
  **Entradas**. Una secuencia de pares $F = [(c_1, f_1), (c_2, f_2), \dots, (c_n, f_n)]$ representando la tabla de caracteres y sus correspondientes frecuencias.  
  **Salida**. Un árbol de Huffman que contiene los datos de $F$.  

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de pares $F = [(c_1, f_1), (c_2, f_2), \dots, (c_n, f_n)]$ representando la tabla de caracteres y sus correspondientes frecuencias.}
    \KwOutput{Un árbol de Huffman que contiene los datos de $F$.}
  
    \BlankLine
    \caption{Construir un árbol de Huffman}
    \SetAlgoVlined

    $n \assign |F|$ \;
    $Q \assign $ nueva cola de prioridad \;

    \For{$par \in F$} {
        $N \assign $ nuevo nodo \;
        $N.f \assign par.f$ \;
        $N.c \assign par.c$ \;
        $N \twoheadrightarrow Q$ \;
    }

    \While{$|Q| > 1$} {
        $N \assign $ nuevo nodo \;
        $A \twoheadleftarrow Q$ \;
        $B \twoheadleftarrow Q$ \;

        $N.izq \assign A$ \;
        $N.der \assign B$ \;
        $N.f \assign A.f + B.f$ \;

        $N \twoheadrightarrow Q$ \;
    }
    
    $árbol \twoheadleftarrow Q$ \;
    \Return{$árbol$} \;
\end{algorithm}

El análisis de comlejidad temporal de este algoritmo depende de la implementación de cola de prioridad que utilicemos. El algoritmo tiene dos ciclos, uno para llenar la cola y otro para construir el árbol.

Algunas colas presentan complejidad temporal $\Theta(1)$ para la operación de insertar mientras que otras presentan complejidad temporal $\mathcal{O}(\log n)$ para la misma operación. De esta forma, escogiendo la cola más eficiente, el ciclo de llenado de la cola (líneas 3-7) podría presentar complejidad temporal de $\mathcal{O}(n)$, puesto que en cada iteración del recorrido lineal realizaría $\mathcal{O}(1)$ operaciones. Mientras que escogiendo la cola menos eficiente en inserciones, tendríamos una complejidad temporal de $\mathcal{O}(n) \cdot \mathcal{O}(\log n) = \mathcal{O}(n \log n)$.

Por otro lado, la operación de eliminar de la cola o desencolar, es al menos $\mathcal{O}(\log n)$ para todas las colas. Por tanto, para el ciclo de construcción del árbol (líneas 8-15) tenemos un recorrido lineal que desencola en cada iteración, entonces podemos esperar una complejidad temporal de $\mathcal{O}(n \log n)$.

## Algoritmo LZW ##

Este es un algoritmo de codificación adaptativa. Esto quiere decir que el algoritmo irá construyendo la nueva codificación comprimida según vaya leyendo la secuencia de entrada. 

Esta codificación se representa en una tabla que se construye dinámicamente. La tabla dirige tanto el proceso de compresión como el de descompresión. La característica de este algoritmo es que la tabla se construye durante la compresión con base en la información obtenida de la secuencia de entrada, y también se construye durante la descompresión también utilizando la información obtenida de la secuencia de entrada comprimida. Por tanto, nunca es necesario transmitir o incluir la tabla como parte de las entradas del algoritmo.

Este algoritmo requiere que conozcamos de antemano cada uno de los símbolos posibles en el alfabeto a partir del cual se construirán las secuencias que serán comprimidas. Por ejemplo si queremos comprimir secuencias de ADN sabemos que el alfabeto será $\{A, C, G, T\}$; por otro lado se deseamos comprimir texto en lenguaje natural podríamos utilizar el alfabeto de los 256 caracteres en codificación ASCII. 

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de caracteres $E$}
    \KwOutput{Una secuencia de números naturales que es una codificación que representa la misma secuencia de datos que $E$}
    \BlankLine
    \caption{Comprimir LZW}
    \SetAlgoVlined
    $T \assign$ inicializar tabla con cada símbolo del alfabeto como llave y un código secuencial como valor \;
    $código \assign$ número de símbolos distintos en el alfabeto\;
    \BlankLine
    $última \assign ``"$\;
    $salida \assign [\ ]$ \;

    \For{$s \in E$} {

        $secuencia \assign última + s$ \;

        \If{$secuencia \in T$} {
            $última \assign secuencia$ \;
        } \Else {
            $salida \twoheadleftarrow T[última]$ \;
            $T[secuencia] \assign código$ \;
            $código \assign código + 1$ \;
            $última \assign s$ \;
        }
    }

    $salida \twoheadleftarrow T[última]$ \;

    \Return{salida} \;
\end{algorithm}

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de números naturales $E$}
    \KwOutput{Una secuencia de caracteres que es una codificación que representa la misma secuencia de datos que $E$}
    \BlankLine
    \caption{Descomprimir LZW}
    \SetAlgoVlined
    $T \assign$ inicializar tabla con una secuencia de códigos como llaves y cada símbolo del alfabeto como valores correspondientes\;
    $sigcod \assign$ número de símbolos distintos en el alfabeto\;
    \BlankLine
    $decodificada \assign T[\pop(E)]$ \;
    $anterior \assign decodificada$ \;
    $salida \twoheadleftarrow decodificada$ \;

    \For{$código \in E$} {
        \If{$código \in T$} {
            $decodificada \assign T[código]$ \;
        } \ElseIf{$código = sigcod$} {
            $decodificada \assign anterior + decodificada[0]$ \;
        }

        $salida \twoheadleftarrow decodificada$ \;
        $T[sigcod] \assign anterior + decodificada[0]$ \;
        $sigcod \assign sigcod + 1$ \;
        $anterior \assign decodificada$ \;
    }

    \Return{salida} \;
\end{algorithm}



## Comparación entre LZW y Huffman ##

Ambos algoritmos son voraces. La voracidad de Huffman se da al intentar asociar los códigos más cortos a los símbolos más frecuentes. Por otro lado, la voracidad de LZW se debe a que siempre busca generar la secuencia no registrada en la tabla más larga posible.

El algoritmo de Huffman requiere que conozcamos las frecuencias de ocurrencia de los distintos símbolos que componen la secuencia de entrada a comprimir. Este requerimiento implica que es necesario hacer un recorrido por la entrada construyendo la tabla de frecuencias, para luego construir el árbol con la codificación, y finalmente recodificar la entrada en un segundo recorrido. 

Por otro lado, LZW requiere que conozcamos el alfabeto de todos los símbolos posibles a utilizar en la secuencia de entrada, lo cual no es un reto particular pues basta con tomar en cuenta todos los posibles caracteres del estándar de codificación de hileras que estemos utilizando.

Los árboles de codificación de Huffman deben ser incluidos en el archivo comprimido o transmitidos junto con la secuencia comprimida para que el algoritmo de descompresión pueda reutilizarlo para transformar la codificación. Al contrario, en LZW la tabla puede ser reconstruida por el algoritmo de descompresión con base en la propia secuencia de entrada y por tanto no debe ser transmitida.

El algoritmo LZW es recomendable cuando la secuencia a comprimir contiene patrones que se repiten constantemente, por ejemplo una secuencia de ADN. Esto por que la tabla identifica fragmentos de la secuencia, en la medida en que estos fragmentos se repitan los códigos podrán ser reutilizados, requiriendo por tanto menos códigos para representar más símbolos. Si los fragmentos no se repiten, el mapeo de la codificación original a la nueva codificación se aproximará más a una relación 1:1.

Por otro lado, el algoritmo de Huffman es recomendable para codificar textos en algún lenguaje donde no necesariamente haya repetición constante de fragmentos, pero si hay una distribución probabilística no uniforme de los distintos símbolos que componen su alfabeto. Por ejemplo en textos escritos en algún lenguaje natural como el español, el sánscrito o el bribri.


## Referencias ##

Balakrishnan, H (2012) Compression Algorithms: Huffman and Lempel-Ziv-Welch (LZW) [notas de clase]. Disponible en: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/readings/MIT6_02F12_chap03.pdf

Contribuidores a RosettaCode (2020) LZW compression. En *Rosetta Code*. Recuperado en diciembre 2020 de: http://rosettacode.org/wiki/LZW_compression

Cormen T., Leiserson C., Rivest R. y Stein C. (2009) Introduction to Algorithms (2da ed.). MIT Press.
